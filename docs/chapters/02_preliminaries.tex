% !TeX root = ../main.tex
% Add the above to each chapter to make compiling the PDF easier in some editors.
\chapter{Prelimiaries}\label{chapter:prelimiaries}

\theoremstyle{definition}
\newtheorem{definition}{Definition}[section]

\theoremstyle{remark}
\newtheorem{example}{Example}[section]

In this section, we introduce the notation used throughout the paper and capture the most important preliminaries in definitions. We start with the mathematical notation and concepts used in this paper.

\section{Mathematical Prelimiaries}

We let $p$ and $q$ denote prime numbers if not explicitly stated otherwise. 
Groups are written in a multiplicate manner with the  $\cdot$ operator and the abbreviation $ab$ for $a \cdot b$. We let $g$ and $h$ denote group elements if not explicitly stated otherwise.
Furthermore, we use the notation $\mathbb{F}_p$ for a finite field of prime order p (note that the integers modulo p are isomorph to any finite field of prime order p \parencite{algebra}) with the conventional operators `+` and `$\cdot$` for addition and multiplication. We let $a$,$b$, and $c$ denote finite field elements if not explicitly stated otherwise.

\begin{definition}[cyclic group]
Let $\mathbb{G}$ be a group of prime order p. We call a group cyclic iff: $\exists g \in \mathbb{G}. \ \forall e \in \mathbb{G}. \ \exists n \in \mathbb{N}. \ e = g^n$, which is equivalent to $\mathbb{G} = \{1,g,g^2,...,g^{p-1}\}$ \parencite{algebra}. If such a $g$ exists, we call it a generator.

From now on we write \textbf{g} for a randomly chosen but fixed generator of a respective cyclic group. 
\end{definition}

\begin{definition}[pairings]
    \label{pairings_def}
    Let $\mathbb{G}$ and $\mathbb{H}$ be two groups of prime order p. A pairing is a function: $\mathbb{G} \times \mathbb{G} \rightarrow \mathbb{H}$, with the following two properties:
    \begin{itemize}
        \item \textbf{Bilineraity:} $\forall g,h \in \mathbb{G}. \ \forall a,b \in \mathbb{F}_p. \ e(g^a,h^b) = e(g,h)^{ab}$
        \item \textbf{Non-degeneracy:} $\neg (\forall g,h \in \mathbb{G}. \ e(g,h)=1)$
    \end{itemize}
    \parencite{KZG}
\end{definition}

From now on let $e$ denote a pairing function if not explicitly stated otherwise.

Now that we have introduced the mathematical preliminaries we will tend to the cryptographic preliminaries. 

\section{Cryptographic Prelimiaries}
In this section, we will introduce the security notions that we use in this paper and the concepts behind them.

We start with the definition of a negligible and a poly-bounded function from which we will define our adversarial model, against which we will prove security in this paper.

\begin{definition}
    Let $f: \mathbb{Z}_{\ge 0} \rightarrow \mathbb{R}$ be a function. We call $f$ negligible iff:
    \begin{equation*}
        \forall c \in \mathbb{R}_{> 0}. \ \exists n_0. \ \forall n \ge n_0. \ \vert f(n)\vert < 1/n^c
    \end{equation*}
    \parencite{boneh_shoup}
\end{definition}
Boneh and Shoup state "Intuitively, a negligible function $f:\mathbb{F}_{\ge 0} \rightarrow \mathbb{R}$ is one that not only tends to zero as $n \rightarrow \infty$, but
does so faster than the inverse of any polynomial." \parencite{boneh_shoup}

From now on let $\epsilon$ denote a negligible function if not explicitly stated otherwise. 

\begin{definition}
    Let $f: \mathbb{Z}_{\ge 0} \rightarrow \mathbb{R}$ be a function. We call $f$ poly-bounded iff:
    \begin{equation*}
        \exists c,d \in \mathbb{R}_{>0}. \ \forall n \in \mathbb{N}_0. \ \vert f(n)\vert \le n^c+d
    \end{equation*}
    \parencite{boneh_shoup}
\end{definition}


Note, we will define (probabilistic) algorithms for some security parameter $\kappa$ and bound their performance using the notion of negligibility and poly-boundedness with respect to $\kappa$.

We capture the security of our cryptographic system in games against an (efficient) adversary. Typically, the adversary has to break a security property in those games (e.g. decrypt a cyphertext). However, before we formally define games, we define what an adversary is.

\begin{definition}[Efficient Adversary]
    \label{Adversary}
An adversary is a probabilistic algorithm, that takes a security parameter $\kappa$ as its first argument and returns some probabilistic result. 
We call an adversary efficient if its running time is poly-bounded in $\kappa$ except for negligible probability (with respect to $\kappa$)
\parencite{boneh_shoup}.
\end{definition}

Besides this definition, we will use a stronger definition of adversaries, namely that of the adversary in the Algebraic Group Model (AGM) \parencite{AGM}.

\begin{definition}[AGM Adversary]
    Let $\mathbb{F}_p$ be a finite field of prime order p and $\mathbb{G}$ a cyclic group of prime order p. An adversary in the AGM is an adversary as in \ref{Adversary}, that furthermore outputs a vector $\vec{v}_e \in \mathbb{F}_p^t$ for every element $e \in \mathbb{G}$ in its output, such that $e = \prod_{1}^{t} g_i^{v_{e_i}}$, where $g\in \mathbb{G}^t$ is the vector of all elements of $\mathbb{G}$ that the adversary has seen so far
    \parencite{AGM}. 

    The efficiency definition is analogue to \ref*{Adversary}
\end{definition}

Now that we have defined adversary models, we define games. 

\begin{definition}[games]
Games are probabilistic algorithms with access to an adversary \parencite*{boneh_shoup}. They output a boolean value that represents either that the game has been won by the adversary or that it has been lost \parencite{boneh_shoup}. Formally we write games as a sequence of functions and adversary calls \parencite{boneh_shoup}.
\end{definition}

Notationwise, we write $`\overset{{\scriptscriptstyle\$}}{\leftarrow}`$ followed by a set for uniform sampling from that set, $`\leftarrow`$ followed by a probability mass function (e.g. an adversary result) to sample from that function space, and $`=`$ for an assignment of a deterministic value. Moreover, we write $`:`$ followed by a condition to assure that the condition has to hold at this point. To give an example, think of the following game as "sampling a uniformly random $a$ from $\mathbb{F}_p$, get the probabilistic result from $\mathcal{A}$ as $b$, computing $c$ as $F$ applied to $a$ and $b$, and assert that $P$ holds for $c$":
\begin{equation*}
    \left(
    \begin{aligned}
        a & \overset{{\scriptscriptstyle\$}}{\leftarrow} \mathbb{F}_p, \\
        b & \leftarrow \mathcal{A}, \\
        c & = F(a,b) \\
        & : \ P(c)
    \end{aligned}
    \right)
\end{equation*}
Note, that the notation is applicable for any probabilistic algorithms, not just games. However, if not explicitly stated otherwise we will let this notation denote games.

Next, we define game-based proofs, the method which we will use to formally prove security.

\begin{definition}[game-based proofs]
    \label{game-hops-def}
    Game-based proofs are a sequence of game-hops that bound the probability of one game to another \parencite{gamesB&R,shoup_games}.

    The two types of game hops we will use in our proofs are:
    \begin{itemize}
        \item \textbf{game hop as a bridging step:} 
        
        A bridging step alters the function definitions, such that the game probability does not change \parencite{shoup_games}.
        \item \textbf{game hop based on a failure event:}
        
        In a game hop based on a failure event, two games are equal except if a specific failure event occurs \parencite{shoup_games}. The failure event should have a negligible probability for the game-based proof to hold.
    \end{itemize}
\end{definition}

Typically we will define a game for a certain security definition applied to our cryptographic protocol and reduce that game using game hops to a hardness assumption game, thus showing that breaking the security definition for our cryptographic protocol is at least as hard as breaking the hardness assumption \parencite{boneh_shoup}. Hence we need to define hardness and accordingly hardness assumptions:

\begin{definition}[hardness]
    Given a computational problem P, we say P is hard if and only if no efficient adversary exists, that solves P with non-negligible probability \parencite{ac_handbook}. 
\end{definition}

\begin{definition}[hardness assumptions]
    \label{hardness_asmp_defs}
Hardness assumptions are computational problems that are generally believed to be hard \parencite{boneh_shoup,ac_handbook}.
\end{definition}

Within cryptography, there exist several hardness assumptions, we will cover the ones used in this paper and formally define according games. 

From now on let `$\overset{{\scriptscriptstyle\$}}{\leftarrow}$` denote uniform sampling from the respective set. 

\begin{definition}[discrete logarithm (DL)]
    \label{DL_def}
    Let $\mathbb{G}$ be a cyclic group with generator \textbf{g}.
    Let $a \overset{{\scriptscriptstyle\$}}{\leftarrow} \mathbb{F}_p$. Then for every adversary $\mathcal{A}: \text{Pr}[a = \mathcal{A}(\textbf{g}^a)] = \epsilon$  holds \parencite{KZG}.

    Formally we define the DL game: 
    \begin{equation*}
        \left(
            \begin{aligned}
                a & \overset{{\scriptscriptstyle\$}}{\leftarrow} \mathbb{F}_p \\
                a' & \leftarrow \mathcal{A}(\textbf{g}^a) \\
                & : a = a'
            \end{aligned}
        \right)
    \end{equation*}
\end{definition}

\begin{definition}[t-Strong Diffie Hellmann (t-SDH)]
    \label{tSDH_def}
    Let $\mathbb{G}$ be a cyclic group with generator \textbf{g}.
    Let $t \in \mathbb{N}$ be fixed.  Let $a \overset{{\scriptscriptstyle\$}}{\leftarrow} \mathbb{F}_p$. For every adversary $\mathcal{A}:$
    \begin{equation*}
        \text{Pr}\big[
            (c,\textbf{g}^{\frac{1}{a+c}}) = \mathcal{A}([\textbf{g},\textbf{g}^a,\textbf{g}^{a^2},\dots, \textbf{g}^{a^{t-1}}])
        \big] = \epsilon
    \end{equation*}
    holds for all $c \in \mathbb{F}_p\backslash \{a\}$ \parencite{KZG}.
    
    Formally we define the t-SDH game: 
    \begin{equation*}
        \left(
            \begin{aligned}
                a & \overset{{\scriptscriptstyle\$}}{\leftarrow} \mathbb{F}_p \\
                (c,g') & \leftarrow \mathcal{A}([\textbf{g},\textbf{g}^a,\textbf{g}^{a^2},\dots, \textbf{g}^{a^{t-1}}]) \\
                & : \textbf{g}^{\frac{1}{a+c}} = g'
            \end{aligned}
        \right)
    \end{equation*}
\end{definition}

The following definition is analogous to the previous one, except that the result is passed through a pairing function. Intuitively, this makes this assumption stronger as two groups, and thus group values, and the pairing function can now be used by an adversary.

\begin{definition}[t-Bilinear Strong Diffie Hellmann (t-BSDH)]
    \label{tBSDH_def}
    Let $\mathbb{G}$ and $\mathbb{H}$ be cyclic groups with generators \textbf{g} and \textbf{h}.
    Let $t \in \mathbb{N}$ be fixed and $e: \mathbb{G} \times \mathbb{G} \rightarrow \mathbb{H}$ be a pairing function.  Let $a \overset{{\scriptscriptstyle\$}}{\leftarrow} \mathbb{F}_p$. For every adversary $\mathcal{A}:$
    \begin{equation*}
        \text{Pr}\big[
            (c,e(\textbf{g}, \textbf{g} )^{\frac{1}{a+c}}) = \mathcal{A}([\textbf{g},\textbf{g}^a,\textbf{g}^{a^2},\dots, \textbf{g}^{a^{t-1}}])
        \big] = \epsilon
    \end{equation*}
    holds for all $c \in \mathbb{F}_p\backslash \{a\}$ \parencite{KZG}.
    
    Formally we define the t-BSDH game: 
    \begin{equation*}
        \left(
            \begin{aligned}
                a & \overset{{\scriptscriptstyle\$}}{\leftarrow} \mathbb{F}_p \\
                (c,g') & \leftarrow \mathcal{A}([\textbf{g},\textbf{g}^a,\textbf{g}^{a^2},\dots, \textbf{g}^{a^{t-1}}]) \\
                & : e(\textbf{g}, \textbf{g})^{\frac{1}{a+c}} = g'
            \end{aligned}
        \right)
    \end{equation*}
\end{definition}

Now that we have introduced the necessary preliminaries, notions, and definitions for proving security for cryptographic protocols, we tend to the type of protocols we formalize in this work.

\begin{definition}[Commitment Schemes (CS)]
    \label{CS}
    A Commitment Scheme is a cryptographic protocol between two parties, we call the committer and the verifier, and consists of three functions:
    \begin{itemize}
        \item \textbf{KeyGen} generates a key $ck$ for the committer and a key $vk$ for the verifier.
        \item \textbf{Commit}
        takes the committer key $ck$, a message $m$ and computes a commitment $C$ for $m$ and a opening value $ov$. 
        \item \textbf{Verify} 
        takes the verifier key $vk$, a commitment $C$, an opening value $ov$, a message $m$ and decides whether $C$ is a valid commitment to $m$ using $vk$ and $ov$.
    \end{itemize}
    \parencite{thalerbook}

    The protocol assumes that KeyGen was used to distribute the keys $ck$ and $vk$ accordingly to the committer and verifier, such that no party can learn the keys of the other party if they are to remain secret. 

    Once the keys are correctly distributed, the protocol follows three steps: 
    \begin{enumerate}
        \item The committer uses their keys $ck$ to commit to a arbitrary message $m$, invoking Commit$(ck,m)$ from which they obtain a commitment $C$ to $m$ and an opening value $ov$ for the commitment. The committer stores $ov$ and sends $C$ to the verifier.
        \item At a later point in time, the committer might decide to open the commitment $C$ they sent to the verifier. To open the commitment the committer sends the opening value $ov$ and the message $m$ to the verifier. 
        \item The verifier invokes Verify on the values it received from the committer ($C,m$ and $ov$) to decide whether $m$ is the message the commitment $C$ was computed for. 
    \end{enumerate}
    Once the keys are set up, the protocol may run arbitrary times and in parallel (i.e. the committer can commit to arbitrary many messages and reveal them independently).
    % TODO Grafik f√ºr CS hier
\end{definition}

In our formalization, we work with a specific type of commitment scheme, namely Polynomial Commitment Schemes (PCS): 

\begin{definition}[Polynomial Commitment Scheme (PCS)]
    \label{PCS_def}
    A Polynomial Commitment Scheme is a Commitment Scheme as defined in \ref*{CS}, with its message space restricted to polynomials (i.e. messages are polynomials). 
    
    Furthermore, a PCS supports two more functions to allow point-wise (i.e. partly) opening a commitment to a polynomial: 
    \begin{itemize}
        \item \textbf{CreateWitness}
        takes the committer key $ck$, a polynomial $\phi$, a value $i$ and computes a witness $w$ for the point $(i, \phi(i))$. 
        \item \textbf{VerifyEval:} 
        takes the verifier keys $vk$, a commitment $C$, a point $(i, \phi(i))$, a witness $w$ and checks that the point is consistent with the commitment (i.e. the point is part of the polynomial that $C$ is a commitment to) using $w$ and $vk$.
    \end{itemize}
\end{definition}

Note that we omit the verifier keys in the following four definitions for readability, but generally assume the adversaries to have access to the verifier keys. 

We formally define four security properties for a PCS:

\begin{definition}[Polynomial Binding]
    We say a PCS is polynomial binding if and only if the probability of any efficient adversary finding a commitment value $C$, opening values $ov$ and $ov'$ and polynomials $\phi$ and $\phi'$, such that:
    \begin{equation*}
        \text{Pr}\big[
            \text{Verify}(C,ov,\phi) \land \text{Verify}(C,ov',\phi')
        \big]
        = \epsilon
    \end{equation*}
    \parencite{KZG}
\end{definition}

\begin{definition}[Evaluation Binding]
    We say a PCS is evaluation binding if and only if the probability of any efficient adversary finding a commitment value $C$, two witnesses $w$ and $w'$ and two evaluations $\phi(i)$ and $\phi(i)'$ for any $i$, such that:
    \begin{equation*}
        \text{Pr}\big[
            \text{VerifyEval}(C,(i,\phi(i)), w) \land \text{VerifyEval}(C,(i,\phi(i)'), w')
        \big]
        = \epsilon
    \end{equation*}
    \parencite{KZG}
\end{definition}

\begin{definition}[Knowledge Soundness (AGM)]
    \label{knowledgesound_def}
    We say a PCS is knowledge sound in the AGM if and only if there exists an efficient extractor algorithm $E$ such that for every efficient adversary $\mathcal{A}=(\mathcal{A}_1, \mathcal{A}_2)$ in the AGM the probability of winning the following game is negligible: 
    \begin{equation*}
        \left(
            \begin{aligned}
                (ck,vk) & \leftarrow \text{KeyGen} \\
                (C,\vec{v}_C, \sigma) & \leftarrow \mathcal{A}_1(ck)\\
                p & \leftarrow E(C,\vec{v}) \\
                (i, \phi(i), \omega, \vec{v}_{\omega}) &\leftarrow \mathcal{A}_2(\sigma)\\
                & : \phi(i) \ne p(i) \land \\
                & \hspace{3.5mm} \text{VerifyEval}(vk, C,(i,\phi(i)),w)
            \end{aligned}
        \right)
    \end{equation*}
    \parencite{plonk}
\end{definition}

\begin{definition}[hiding]
    \label{hiding_def}
    We say a PCS is hiding if and only if the probability of any efficient adversary finding an unknown point of the polynomial $\phi$ from the commitment $C:=\text{Commit}(ck,\phi)$ and deg$(\phi)-1$ points with witness  $(i,\phi(i),\text{CreateWitness}(ck, \phi, i))$ is negligible.
    \parencite{KZG}
\end{definition}

\section{Isabelle/HOL}
Now that we have covered the theoretical preliminaries, we introduce Isabelle/HOL\parencite{Isabelle}, the interactive theorem prover we use to formalize our proofs. 
Note that we will use the abbreviation \textit{Isabelle} to refer to \textit{Isabelle/HOL} from now on. 

Isabelle is an interactive theorem prover for higher-order logic (HOL) that supports a minimal functional programming language \parencite{isabelle_manual}, as well as a large set of formalizations of mathematical areas such as algebra and analysis. The most essential formalizations are shipped with Isabelle in the \textit{HOL-library}, besides that there exists a large database of formalizations, the \textit{Archive of Formal Proofs (AFP)}\parencite{AFP_online}. We will use the finite field definition as well as the factorization algorithm for polynomials over finite fields from the Berlekamp-Zassenhaus\parencite{Berlekamp_Zassenhaus-AFP} AFP entry. Furthermore, we use the Lagrange Polynomial Interpolation algorithm from the AFP entry \textit{Polynomial Interpolation}\parencite{Polynomial_Interpolation-AFP}.
Apart from that, we use another AFP entry that is central to our proofs, the CryptHOL framework\parencite{CryptHOL-AFP}.

\subsection*{CryptHOL}
CryptHOL is a framework for game-based proofs in Isabelle \parencite{CryptHOL_game_based}. It captures games in a sub-probability mass function (spmf) monad. That is essentially a \textit{Option} type wrapped in a probability mass function (pmf). The unassigned probability mass, represented as the pmf of \textit{None} is propagated through the monad.
Elementary events from spmf's can be bound to variables through the monadic notation. 
The notation for games in CryptHOL is drawn from Haskell's do-notation.  
\begin{example}
    \hspace{0mm}
    \begin{lstlisting}[language=isabelle]
        do {
            x $\leftarrow$ sample_uniform S;
            return_spmf x
        }
    \end{lstlisting}

    The \textit{do \{\dots\}} block is syntactic sugar, intuitively, it captures the monadic block (i.e. a game).
    \textit{sample\_uniform} is the spmf, that wraps the uniformly over S distributed pmf.
    We use $\leftarrow$ to bind an elementary event of the pmf to x, intuitively this means x is not a concrete element of S, but represents any element of S with respect to the probability distribution of the pmf (which in the example is uniform). Note, that if S is empty, the binding operation fails and the result is the unassigned probability mass. 
    We use \textit{return\_spmf} to return the pmf, that x represents, wrapped in a spmf, which is \textit{sample\_uniform} of S in this example. Hence, the result in this example is, in fact, equivalent to the spmf '\textit{sample\_uniform} S'.
\end{example}

More complex games can be created by composing more functions, including functions that fail, i.e. return the unassigned probability mass, on purpose to perform checks. One specific example of such a function is the \textit{assert\_spmf} function, which is commonly used to check that messages from the adversary are well-formed. We illustrate the correct usage in example \ref{assert_spmf_exmpl}. 

CryptHOL furthermore provides syntactic sugar to handle failure (i.e. an unassigned probability mass), namely the \textit{TRY ELSE} block. \textit{TRY A ELSE B}, captures semantically 'try to return $A$ if $A$ is a failure, return $B$'. We will commonly use this pattern to abstract whether an adversary has won a game. Specifically, we define $A$, such that the result is a wrapped spmf of \textit{True} if and only if the adversary has won and otherwise let the game fail, i.e. result in the unassigned probability mass. We define then $B$ to be the wrapped spmf of \textit{False}, so the game \textit{TRY A ELSE B} captures cleanly whether the adversary has won the game.

\begin{example}
    \label{assert_spmf_exmpl}
    \hspace{0mm}
    \begin{lstlisting}[language=isabelle]
        TRY do {
            x::nat $\leftarrow$ sample_uniform S;
            x'::nat $\leftarrow \mathcal{A}$ ($\mathbf{g}$^$x$);
            _::unit $\leftarrow$ assert_spmf(x=x');
            return_spmf True
        } ELSE (return_spmf False)
    \end{lstlisting}

    This example game captures the discrete log problem.

    As in the first game, an elementary event of S, which is a natural number, is bound to x. The adversary is applied to the group element $\mathbf{g}^x$ and returns an spmf over the natural numbers, which we bind to x'.
    Next, we use \textit{assert\_spmf} to check whether x and x' are equal i.e. the adversary guessed the right number. We bind the result only symbolically to \textit{unit}, such that if the check does not hold, the failure can be propagated as the unassigned probability mass. If the assert check holds, the result is the wrapped spmf of \textit{True}, otherwise a failure has been propagated and the \textit{ELSE} branch is triggered. 
    If the \textit{ELSE} branch is triggered the result is the wrapped spmf of \textit{False}.

\end{example}